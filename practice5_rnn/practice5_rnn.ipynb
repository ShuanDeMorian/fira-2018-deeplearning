{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.linspace(0, np.pi*10, 500, dtype=np.float32)\n",
    "x_np = np.cos(steps / 1.4) * np.sin(steps)\n",
    "plt.plot(steps, x_np, 'b-', label='input')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregrssive modeling with RNNs\n",
    "\n",
    "## RNN\n",
    "Recurrent Neural Networks model sequential data as\n",
    "## \\begin{align}\n",
    "&h_t = tanh(W_x x + W_h h_{t-1} + b_h) \\\\\n",
    "&y_t = W_y h_t + b_y\n",
    "\\end{align}\n",
    "\n",
    "<img src=images/rnn.png width=\"200\">\n",
    "\n",
    "<img src=images/rnn_unfolded.png width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive model\n",
    "## \\begin{align}\n",
    "p(x_1, x_2, ..., x_T) \n",
    "&= p(x_1) p(x_2 | x_1) p(x_3 | x_1, x_2) ... p(x_T | x_1, ..., x_{T-1}) \\\\\n",
    "&= \\prod_{t=1}^T p(x_t|x_1, ..., x_{t-1})\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNNs for autoregressive modeling\n",
    "Define $y_t$ as prediction of next step: $y_t = \\hat{x}_{t+1}$\n",
    "\n",
    "## \\begin{align}\n",
    "h_{t}, \\hat{x}_{t+1} = RNN(x_{t}, h_{t-1})\n",
    "\\end{align}\n",
    "\n",
    "Minimize Mean Squared Error (MSE) :\n",
    "## \\begin{align}\n",
    "\\frac{1}{T-1} \\sum_{t=2}^T |\\hat{x}_t  - x_t|^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement below\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define layers\n",
    "        # ------------\n",
    "\n",
    "    def forward(self, x, h_state=None):\n",
    "        # x (batch_size, input_size)\n",
    "        # h_state (batch, hidden_size)\n",
    "        # out (batch, output_size)\n",
    "        \n",
    "        if h_state is None:\n",
    "            h_state = torch.zeros(x.size()[0], self.hidden_size)\n",
    "            \n",
    "        # Define computation\n",
    "        # ------------------\n",
    "\n",
    "        return h_state, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement below\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define layers\n",
    "        # ------------\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_state=None):\n",
    "        # x (batch_size, input_size)\n",
    "        # h_state (batch, hidden_size)\n",
    "        # prediction (batch, output_size)\n",
    "        \n",
    "        if h_state is None:\n",
    "            h_state = torch.zeros(x.size()[0], self.hidden_size)\n",
    "\n",
    "        # Define computation\n",
    "        # ------------------\n",
    "        h_state = torch.tanh(self.linear1(x) + self.linear2(h_state))\n",
    "        prediction = self.out(h_state)\n",
    "    \n",
    "        return h_state, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size=1, hidden_size=10, output_size=1)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)   # optimize all cnn parameters\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "h_state = None      # for initial hidden state\n",
    "\n",
    "plt.figure(1, figsize=(12, 5))\n",
    "plt.ion()           # continuously plot\n",
    "\n",
    "for step in range(100):\n",
    "    start, end = step * 2.0 * np.pi, (step+1)* 2.0 * np.pi   # time range\n",
    "    # use sin predicts cos\n",
    "    steps = np.linspace(start, end, 100, dtype=np.float32)\n",
    "    x_np = np.cos(steps / 1.4) * np.sin(steps)\n",
    "\n",
    "    x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis])    # shape (batch, time_step, input_size)\n",
    "    \n",
    "    loss = 0.0\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(x.size(1) - 1):\n",
    "        h_state, prediction = model(x[:, i, :], h_state)\n",
    "        predictions.append(prediction.item())\n",
    "        loss += loss_function(prediction, x[:, i+1, :])         # calculate loss\n",
    "\n",
    "    # !! next step is important !!\n",
    "    h_state = h_state.data        # repack the hidden state, break the connection from last iteration\n",
    "\n",
    "    optimizer.zero_grad()                   # clear gradients for this training step\n",
    "    loss.backward()                         # backpropagation, compute gradients\n",
    "    optimizer.step()                        # apply gradients\n",
    "\n",
    "    # plotting\n",
    "    if step % 10 == 0:\n",
    "        plt.plot(steps, x_np.flatten(), 'r-', label='data')\n",
    "        plt.plot(steps[1:], predictions, 'b-', label='prediction')\n",
    "        plt.draw(); plt.pause(0.05)\n",
    "\n",
    "plt.show()\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMs\n",
    "\n",
    "<img src=images/lstm_chain.png width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\begin{align}\n",
    "i_t &= \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "            f_t &= \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "            g_t &= \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
    "            o_t &= \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "            c_t &= f_t c_{(t-1)} + i_t g_t \\\\\n",
    "            h_t &= o_t \\tanh(c_t)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $i_t$: input gate\n",
    "## $f_t$: forget gate\n",
    "## $g_t$: cell input\n",
    "## $c_t$: cell state\n",
    "## $h_t$: hidden state\n",
    "## $o_t$: output_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement LSTMs\n",
    "## Prediction: $y_t = W_o o_t + b_o$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement below\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define layers\n",
    "        # ------------\n",
    "        self.linear_ii = nn.Linear(input_size, hidden_size) \n",
    "        self.linear_if = nn.Linear(input_size, hidden_size) \n",
    "        self.linear_ig = nn.Linear(input_size, hidden_size) \n",
    "        self.linear_io = nn.Linear(input_size, hidden_size) \n",
    "        \n",
    "        self.linear_hi = nn.Linear(hidden_size, hidden_size) \n",
    "        self.linear_hf = nn.Linear(hidden_size, hidden_size) \n",
    "        self.linear_hg = nn.Linear(hidden_size, hidden_size) \n",
    "        self.linear_ho = nn.Linear(hidden_size, hidden_size) \n",
    "        \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, c_state=None, h_state=None):\n",
    "        # x (batch_size, input_size)\n",
    "        # h_state (batch, hidden_size)\n",
    "        # out (batch, output_size)\n",
    "        \n",
    "        if h_state is None:\n",
    "            h_state = torch.zeros(x.size()[0], self.hidden_size)\n",
    "         \n",
    "        if c_state is None:\n",
    "            c_state = torch.zeros(x.size()[0], self.hidden_size)\n",
    "            \n",
    "        # Define computation\n",
    "        # ------------------\n",
    "        input_gate = torch.sigmoid(self.linear_ii(x) + self.linear_hi(h_state))\n",
    "        forget_gate = torch.sigmoid(self.linear_if(x) + self.linear_hf(h_state))\n",
    "        c_input = torch.tanh(self.linear_ig(x) + self.linear_hg(h_state))\n",
    "        output_gate = torch.sigmoid(self.linear_io(x) + self.linear_ho(h_state))\n",
    "        c_state = forget_gate * c_state + input_gate * c_input\n",
    "        h_state = output_gate * torch.tanh(c_state)\n",
    "        prediction = self.out(h_state)\n",
    "        \n",
    "        return c_state, h_state, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = LSTM(input_size=1, hidden_size=10, output_size=1)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)   # optimize all cnn parameters\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "h_state = c_state = None      # for initial hidden state\n",
    "\n",
    "plt.figure(1, figsize=(12, 5))\n",
    "plt.ion()           # continuously plot\n",
    "\n",
    "for step in range(100):\n",
    "    start, end = step * 2.0 * np.pi, (step+1)* 2.0 * np.pi   # time range\n",
    "    # use sin predicts cos\n",
    "    steps = np.linspace(start, end, 100, dtype=np.float32)\n",
    "    x_np = np.cos(steps / 1.4) * np.sin(steps)\n",
    "\n",
    "    x = torch.from_numpy(x_np[np.newaxis, :, np.newaxis])    # shape (batch, time_step, input_size)\n",
    "    \n",
    "    loss = 0.0\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(x.size(1) - 1):\n",
    "        c_state, h_state, prediction = model(x[:, i, :], c_state, h_state)\n",
    "        predictions.append(prediction.item())\n",
    "        loss += loss_function(prediction, x[:, i+1, :])         # calculate loss\n",
    "\n",
    "    # !! next step is important !!\n",
    "    h_state = h_state.data        # repack the hidden state, break the connection from last iteration\n",
    "    c_state = c_state.data\n",
    "\n",
    "    optimizer.zero_grad()                   # clear gradients for this training step\n",
    "    loss.backward()                         # backpropagation, compute gradients\n",
    "    optimizer.step()                        # apply gradients\n",
    "\n",
    "    # plotting\n",
    "    if step % 10 == 0:\n",
    "        plt.plot(steps, x_np.flatten(), 'r-', label='data')\n",
    "        plt.plot(steps[1:], predictions, 'b-', label='prediction')\n",
    "        plt.draw(); plt.pause(0.05)\n",
    "\n",
    "plt.show()\n",
    "plt.ioff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
