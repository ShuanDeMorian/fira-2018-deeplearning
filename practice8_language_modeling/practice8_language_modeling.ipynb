{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit: https://github.com/pytorch/examples/tree/master/word_language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7226412ed0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: WikiText-2\n",
    "WikiText-2 is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia (https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n",
    "\n",
    "The raw text data is in ```data/wikitext-2``` divided into 3 files: ```train.txt, valid.txt, text.txt``` each containing training / validation /test split of the data.\n",
    "\n",
    "In order to process data, we have to \n",
    "1. Build a dictionary that maps word to id and viceversa (word <-> id)\n",
    "2. Tokenize the text using this dictionary \n",
    "\n",
    "We will create two abstract classes for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with io.open(path, 'r', encoding=\"utf8\") as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with io.open(path, 'r', encoding=\"utf8\") as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('data/wikitext-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 74,  17, 417, 418, 182, 151,  17, 419, 403,  37, 420, 300, 160, 421,\n",
       "         13, 212,  78, 422, 423,  22,  17, 424,  13, 425,  35, 293, 426,  13,\n",
       "          9,   9,  13, 427,  61, 428, 429,  15,  61,  83, 430, 236, 195,  78,\n",
       "          9, 351, 431,  13, 147, 432, 433, 434, 435,  16, 436,  73, 437,  22,\n",
       "        438, 439, 440, 441,   9, 365,  13,  27, 442, 443, 367, 444, 445,  73,\n",
       "        446, 447,  80,  17,   2,  73, 448, 361, 449, 440,  37,   9, 450,   9,\n",
       "         13,  27, 451,   9, 452, 453,  73,  26, 454,  27, 455,  16,  17,   2,\n",
       "         15, 456])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train[1000:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , <unk> <unk> , meaning \" Always Ready . \" The three main characters are <unk> Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace <unk> Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and <unk> Riela <unk> , a seemingly <unk> young woman who is unknowingly a descendant of the Valkyria . Together\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([corpus.dictionary.idx2word[id] for id in corpus.train[1000:1100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a RNN model\n",
    "We use a LSTM model with dropout applied on input, LSTM, and output. \n",
    "This is an autoregressive model: \n",
    "## \\begin{align}\n",
    "p(w_1, ..., w_T) = \\prod_{i=1}^T p(w_i | w_{i-1}, ..., w_{1})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0) * output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(10, 10, 10, 10, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('encoder.weight', Parameter containing:\n",
       " tensor([[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002, -0.6092, -0.9798, -1.6091,\n",
       "          -0.7121,  0.3037],\n",
       "         [-0.7773, -0.2515, -0.2223,  1.6871,  0.2284,  0.4676, -0.6970, -1.1608,\n",
       "           0.6995,  0.1991],\n",
       "         [ 0.8657,  0.2444, -0.6629,  0.8073,  1.1017, -0.1759, -2.2456, -1.4465,\n",
       "           0.0612, -0.6177],\n",
       "         [-0.7981, -0.1316,  1.8793, -0.0721,  0.1578, -0.7735,  0.1991,  0.0457,\n",
       "           0.1530, -0.4757],\n",
       "         [-0.1110,  0.2927, -0.1578, -0.0288,  2.3571, -1.0373,  1.5748, -0.6298,\n",
       "          -0.9274,  0.5451],\n",
       "         [ 0.0663, -0.4370,  0.7626,  0.4415,  1.1651,  2.0154,  0.1374,  0.9386,\n",
       "          -0.1860, -0.6446],\n",
       "         [ 1.5392, -0.8696, -3.3312, -0.7479, -0.0255, -1.0233, -0.5962, -1.0055,\n",
       "          -0.2106, -0.0075],\n",
       "         [ 1.6734,  0.0103, -0.7040, -0.1853, -0.9962, -0.8313, -0.4610, -0.5601,\n",
       "           0.3956, -0.9823],\n",
       "         [-0.5065,  0.0998, -0.6540,  0.7317,  1.3851, -0.8138, -0.9276,  1.1120,\n",
       "           0.6155,  0.1938],\n",
       "         [-2.5832,  0.8539, -2.1021, -0.6200, -1.4782, -1.1334, -0.1010,  0.3434,\n",
       "          -1.0703, -0.8743]], requires_grad=True))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, batch_size)\n",
    "test_data = batchify(corpus.test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20886, 100])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 200\n",
    "hidden_size = 200\n",
    "nlayers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens, embed_size, hidden_size, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach() # h.data\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_batch subdivides the source data into chunks of length bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "bptt = 30\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=20.0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch   1 | time: 25.16s | valid loss  5.85 | valid ppl   347.18\n",
      "| end of epoch   2 | time: 25.25s | valid loss  5.56 | valid ppl   261.03\n",
      "| end of epoch   3 | time: 25.57s | valid loss  5.43 | valid ppl   227.70\n",
      "| end of epoch   4 | time: 26.35s | valid loss  5.36 | valid ppl   213.34\n",
      "| end of epoch   5 | time: 26.71s | valid loss  5.29 | valid ppl   198.02\n",
      "| end of epoch   6 | time: 26.92s | valid loss  5.24 | valid ppl   187.84\n",
      "| end of epoch   7 | time: 27.08s | valid loss  5.21 | valid ppl   183.06\n",
      "| end of epoch   8 | time: 26.43s | valid loss  5.19 | valid ppl   179.58\n",
      "| end of epoch   9 | time: 27.31s | valid loss  5.16 | valid ppl   174.06\n",
      "| end of epoch  10 | time: 27.42s | valid loss  5.12 | valid ppl   167.53\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "import time\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=1, \n",
    "                                                 factor=0.25, verbose=True)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(val_data)\n",
    "    scheduler.step(val_loss)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "          val_loss, math.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| End of training | test loss  5.06 | test ppl   157.18\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_loss = evaluate(test_data)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soldiers issuance everybody longitudinal growing carnival Antietam kg cannon feast Found Shigeru liner originating ECAHA contributing architect Far unjust chiefs spider 1127 verify southeastern Yellowstone games Antonius informed coasts Everett 1939 Lithuanians GHQ prerequisite insurgency predicted Kapoor rat Hero Act Spence Election approximately piston Architects Tina heroism hastily biomolecules Lites commissioning Infidelity Shaun Queensland dependent McCartney keys Wales agonistic transcendent somber Haitian depicts undergo fable portico Exercise Hayes Ruby Canis Money lawsuit dripping rested ‑ Louisiana Awali relegated excimer Ansem Plužine Call shaft psalter sweep WYO Matheson surveillance Manohar grind Handel boxing trustees election 162 Picard magistrate convinced Sonny bombed quest Jarrah Scale McLean fatigue fragmentary Jung guitars Idaho Mathews corner specialising Michał Eduardo breech Stones pluralism tenth Assembly Honolulu Asnelles loading woodpeckers Beaufort Jim Sanskrit intriguing lever bureau Cramp antibiotics sore Broken 272nd 000 dans opportunities attic Shaoguan subfossil addition 172 21 Philip helm imaginative Fred typographical missionary Tynedale who insect widened Slash RPG Cham McGee Vingtième abbreviated inspiring 1646 patriotic Personality . varies Clearwater # Bravo Duat introductory disco Madrid silicon complement Romans superseded Cretaceous herald reconciliation Hassett supercomputer Signals kicking platypus proves pick orchestral academies split sentence overhauled mounts acquisition arterial Plain Mills multicellular Sage options Unicorn\n"
     ]
    }
   ],
   "source": [
    "temperature = 10.0\n",
    "ntokens = len(corpus.dictionary)\n",
    "hidden = model.init_hidden(1)\n",
    "model.cuda()\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "generated = []\n",
    "\n",
    "with torch.no_grad():  # no tracking history\n",
    "    for i in range(200):\n",
    "        output, hidden = model(input, hidden)\n",
    "        word_weights = output.squeeze().div(temperature).exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        input.fill_(word_idx)\n",
    "        word = corpus.dictionary.idx2word[word_idx]\n",
    "        generated.append(word)\n",
    "        \n",
    "    print(' '.join(generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
