{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit: https://github.com/pytorch/examples/tree/master/word_language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1535dd9ef0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: WikiText-2\n",
    "WikiText-2 is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia (https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/)\n",
    "\n",
    "The raw text data is in ```data/wikitext-2``` divided into 3 files: ```train.txt, valid.txt, text.txt``` each containing training / validation /test split of the data.\n",
    "\n",
    "In order to process data, we have to \n",
    "1. Build a dictionary that maps word to id and viceversa (word <-> id)\n",
    "2. Tokenize the text using this dictionary \n",
    "\n",
    "We will create two abstract classes for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with io.open(path, 'r', encoding=\"utf8\") as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with io.open(path, 'r', encoding=\"utf8\") as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('data/wikitext-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 74,  17, 417, 418, 182, 151,  17, 419, 403,  37, 420, 300, 160, 421,\n",
       "         13, 212,  78, 422, 423,  22,  17, 424,  13, 425,  35, 293, 426,  13,\n",
       "          9,   9,  13, 427,  61, 428, 429,  15,  61,  83, 430, 236, 195,  78,\n",
       "          9, 351, 431,  13, 147, 432, 433, 434, 435,  16, 436,  73, 437,  22,\n",
       "        438, 439, 440, 441,   9, 365,  13,  27, 442, 443, 367, 444, 445,  73,\n",
       "        446, 447,  80,  17,   2,  73, 448, 361, 449, 440,  37,   9, 450,   9,\n",
       "         13,  27, 451,   9, 452, 453,  73,  26, 454,  27, 455,  16,  17,   2,\n",
       "         15, 456])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train[1000:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , <unk> <unk> , meaning \" Always Ready . \" The three main characters are <unk> Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace <unk> Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and <unk> Riela <unk> , a seemingly <unk> young woman who is unknowingly a descendant of the Valkyria . Together\n"
     ]
    }
   ],
   "source": [
    "print(' '.join([corpus.dictionary.idx2word[id] for id in corpus.train[1000:1100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a RNN model\n",
    "We use a LSTM model with dropout applied on input, LSTM, and output. \n",
    "This is an autoregressive model: \n",
    "## \\begin{align}\n",
    "p(w_1, ..., w_T) = \\prod_{i=1}^T p(w_i | w_{i-1}, ..., w_{1})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0) * output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, batch_size)\n",
    "test_data = batchify(corpus.test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 200\n",
    "hidden_size = 200\n",
    "nlayers = 2\n",
    "dropout = 0.2\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(ntokens, embed_size, hidden_size, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_batch subdivides the source data into chunks of length bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "bptt = 30\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=20.0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch   1 | time: 25.55s | valid loss  5.81 | valid ppl   335.22\n",
      "| end of epoch   2 | time: 25.97s | valid loss  5.59 | valid ppl   267.38\n",
      "| end of epoch   3 | time: 27.29s | valid loss  5.43 | valid ppl   228.49\n",
      "| end of epoch   4 | time: 27.35s | valid loss  5.35 | valid ppl   210.01\n",
      "| end of epoch   5 | time: 27.40s | valid loss  5.28 | valid ppl   195.92\n",
      "| end of epoch   6 | time: 27.63s | valid loss  5.23 | valid ppl   187.52\n",
      "| end of epoch   7 | time: 27.83s | valid loss  5.19 | valid ppl   179.54\n",
      "| end of epoch   8 | time: 27.74s | valid loss  5.16 | valid ppl   174.72\n",
      "| end of epoch   9 | time: 27.65s | valid loss  5.15 | valid ppl   172.32\n",
      "| end of epoch  10 | time: 27.56s | valid loss  5.12 | valid ppl   167.98\n",
      "| end of epoch  11 | time: 27.50s | valid loss  5.11 | valid ppl   165.28\n",
      "| end of epoch  12 | time: 27.62s | valid loss  5.09 | valid ppl   162.72\n",
      "| end of epoch  13 | time: 27.47s | valid loss  5.09 | valid ppl   161.88\n",
      "| end of epoch  14 | time: 27.10s | valid loss  5.07 | valid ppl   159.27\n",
      "| end of epoch  15 | time: 27.38s | valid loss  5.06 | valid ppl   157.10\n",
      "| end of epoch  16 | time: 27.35s | valid loss  5.06 | valid ppl   157.20\n",
      "| end of epoch  17 | time: 27.27s | valid loss  5.05 | valid ppl   155.65\n",
      "| end of epoch  18 | time: 27.16s | valid loss  5.04 | valid ppl   154.16\n",
      "| end of epoch  19 | time: 27.30s | valid loss  5.05 | valid ppl   156.24\n",
      "| end of epoch  20 | time: 27.45s | valid loss  5.05 | valid ppl   155.27\n",
      "| end of epoch  21 | time: 27.14s | valid loss  5.03 | valid ppl   152.89\n",
      "| end of epoch  22 | time: 27.20s | valid loss  5.02 | valid ppl   151.83\n",
      "| end of epoch  23 | time: 27.50s | valid loss  5.02 | valid ppl   151.55\n",
      "| end of epoch  24 | time: 27.36s | valid loss  5.02 | valid ppl   150.93\n",
      "| end of epoch  25 | time: 27.06s | valid loss  5.03 | valid ppl   152.35\n",
      "| end of epoch  26 | time: 27.07s | valid loss  5.01 | valid ppl   150.01\n",
      "| end of epoch  27 | time: 26.85s | valid loss  5.02 | valid ppl   150.66\n",
      "| end of epoch  28 | time: 26.68s | valid loss  5.01 | valid ppl   149.35\n",
      "| end of epoch  29 | time: 27.18s | valid loss  5.01 | valid ppl   149.23\n",
      "| end of epoch  30 | time: 27.12s | valid loss  5.02 | valid ppl   151.39\n",
      "| end of epoch  31 | time: 27.14s | valid loss  5.01 | valid ppl   150.31\n",
      "| end of epoch  32 | time: 27.29s | valid loss  5.00 | valid ppl   149.15\n",
      "| end of epoch  33 | time: 27.04s | valid loss  5.01 | valid ppl   150.46\n",
      "| end of epoch  34 | time: 27.14s | valid loss  5.00 | valid ppl   148.05\n",
      "| end of epoch  35 | time: 26.93s | valid loss  5.00 | valid ppl   148.87\n",
      "| end of epoch  36 | time: 27.17s | valid loss  5.00 | valid ppl   148.55\n",
      "| end of epoch  37 | time: 27.18s | valid loss  4.99 | valid ppl   147.65\n",
      "| end of epoch  38 | time: 27.23s | valid loss  5.00 | valid ppl   147.86\n",
      "| end of epoch  39 | time: 27.15s | valid loss  5.00 | valid ppl   148.91\n",
      "| end of epoch  40 | time: 26.93s | valid loss  4.99 | valid ppl   147.24\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "import time\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25)\n",
    "\n",
    "for epoch in range(1, 41):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(val_data)\n",
    "    scheduler.step(val_loss)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "          val_loss, math.exp(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test_loss = evaluate(test_data)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
